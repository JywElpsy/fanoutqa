import hashlib
import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Literal, Optional, Tuple

# prevent manipulation of results - the results must be generated by this script or else the hash will not match
LEADERBOARD_SALT = os.getenv("LEADERBOARD_SALT", "supersecret").encode()
SUBMISSIONS_ROOT = Path(__file__).parent
METADATA_PATH = SUBMISSIONS_ROOT / "metadata"
RESULTS_IN_PATH = SUBMISSIONS_ROOT / "results-pr"
RESULTS_OUT_PATH = SUBMISSIONS_ROOT / "results"
CB_PATH = SUBMISSIONS_ROOT / "closedbook-generations"
OB_PATH = SUBMISSIONS_ROOT / "openbook-generations"
EP_PATH = SUBMISSIONS_ROOT / "evidenceprovided-generations"


@dataclass
class SubmissionMetadata:
    name: str
    authors: str
    url: Optional[str]
    citation: str
    type: Literal["FOUNDATION", "FINETUNE", "PROMPT", "OTHER"]
    context: int
    closedbook_generations: str
    openbook_generations: str
    evidenceprovided_generations: str


def hydrate_all():
    # for each submission file,
    for metadata_fp in METADATA_PATH.glob("*.json"):
        # check if it is valid and needs eval
        try:
            metadata, needs_eval = check_submission(metadata_fp)
        except Exception:
            # if invalid, log a check annotation and mark job failure
            continue  # todo

        # if valid and needs eval, run the eval
        if not needs_eval:
            continue
        print(f"Found submission for {metadata.name} at {metadata_fp.name} that requires eval!")
        print(f"Closed-book generations path: {CB_PATH / metadata.closedbook_generations}")
        print(f"Open-book generations path: {OB_PATH / metadata.openbook_generations}")
        print(f"Evidence-provided generations path: {EP_PATH / metadata.evidenceprovided_generations}")
        eval_submission(metadata_fp, metadata)


def check_submission(metadata_fp: Path) -> Tuple[SubmissionMetadata, bool]:
    """Given a path to a submission metadata file, check if its corresponding results file needs to be regened.

    If exception raised, the input file(s) is/are somehow invalid; log it and continue but don't run evals
    """
    submission_name = metadata_fp.name
    # hash the submission
    the_hash = hashlib.sha256()
    the_hash.update(submission_name.encode())
    the_hash.update(metadata_fp.read_bytes())

    # ensure the metadata is readable and has all the required properties
    with open(metadata_fp) as f:
        metadata_data = json.load(f)
        metadata_data = SubmissionMetadata(**metadata_data)

    # ensure the submission files exist
    # hash the submission files
    cb_fp = CB_PATH / metadata_data.closedbook_generations
    the_hash.update(cb_fp.read_bytes())
    ob_fp = OB_PATH / metadata_data.openbook_generations
    the_hash.update(ob_fp.read_bytes())
    ep_fp = EP_PATH / metadata_data.evidenceprovided_generations
    the_hash.update(ep_fp.read_bytes())

    # salt the hash
    the_hash.update(LEADERBOARD_SALT)

    # check if a results file exists with that hash
    result_fp = RESULTS_IN_PATH / submission_name
    if result_fp.exists():
        with open(result_fp) as f:
            try:
                result_data = json.load(f)
                result_hash = result_data["_submission_hash"]
                if result_hash == the_hash.hexdigest():
                    # if so, no eval needed!
                    return metadata_data, False
            except (ValueError, KeyError):
                return metadata_data, True
    return metadata_data, True


def eval_submission(metadata_fp: Path, metadata: SubmissionMetadata):
    pass


if __name__ == "__main__":
    hydrate_all()
